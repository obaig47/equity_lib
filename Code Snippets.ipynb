{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Drop all columns with no values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop all columns with no values\n",
    "df.dropna(axis=1,how='all',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gspc_formatting(df):\n",
    "    #column index, column name, values, allow duplicates (optional)\n",
    "    df.insert(0, 'MFD', '')\n",
    "    df.insert(0, 'ID', range(1, 1 + len(df)))\n",
    "    df.insert(0, 'Source', 'Source Name')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reasons to exclude"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Duplicate\n",
    "- Invalid Award Amount\n",
    "- Less than 1000\n",
    "- No Vendor\n",
    "- Null Award Amount\n",
    "- Exclusion Category \n",
    "- Award Status"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking if character string substring is in cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def is_mbe(string):\n",
    "    if('MBE' in string):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "dbe['Is MBE'] = False\n",
    "dbe['Is MBE'] = dbe['Certifications'].apply(is_mbe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Disable Autosave"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autosave 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check if string contains number letter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hasNumbers(inputString):\n",
    "    return any(char.isdigit() for char in inputString)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python Index based loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "fruits = ['banana', 'apple',  'mango']\n",
    "\n",
    "for index in range(len(fruits)):\n",
    "    print('Current fruit :', fruits[index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check if item is in an array / list / dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#This works for any collection, not just for lists. For dictionaries, it checks whether the given key is \n",
    "#present in the dictionary.\n",
    "\n",
    "if item in my_list:\n",
    "    # whatever"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Firm Name Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://medium.com/@isma3il/supplier-names-normalization-part1-66c91bb29fc3\n",
    "\n",
    "# Libraries\n",
    "import os, re\n",
    "import pandas as pd\n",
    "import np\n",
    "from cleanco import cleanco\n",
    "# Import supplier names to dataframe\n",
    "df = pd.read_excel('EMEA-spend.xlsx', names=['Supplier_Name'], usecols=[5])\n",
    "# ----------------------------------------\n",
    "# Convert to uppercase\n",
    "df.Supplier_Name_Normalized = df.Supplier_Name_Normalized.str.upper()\n",
    "# Remove commas\n",
    "df.Supplier_Name_Normalized = df.Supplier_Name_Normalized.str.replace(',', '')\n",
    "# Remove hyphens\n",
    "df.Supplier_Name_Normalized = df.Supplier_Name_Normalized.str.replace(' - ', ' ')\n",
    "# Remove text between parenthesis \n",
    "df.Supplier_Name_Normalized = df.Supplier_Name_Normalized.str.replace(r\"\\(.*\\)\",\"\")\n",
    "#\n",
    "df.Supplier_Name_Normalized = df.Supplier_Name_Normalized.str.replace(' AND ', ' & ')\n",
    "# Remove spaces in the begining/end\n",
    "df.Supplier_Name_Normalized = df.Supplier_Name_Normalized.str.strip()\n",
    "# Encode\n",
    "df.Supplier_Name_Normalized = df.Supplier_Name_Normalized.str.encode('utf-8')\n",
    "# Remove business entities extensions (1)\n",
    "df.Supplier_Name_Normalized = df.Supplier_Name_Normalized.apply(lambda x: cleanco(x).clean_name() if type(x)==str else x)\n",
    "# Remove dots\n",
    "df.Supplier_Name_Normalized = df.Supplier_Name_Normalized.str.replace('.', '')\n",
    "# Remove business entities extensions (2) - after removing the dots\n",
    "df.Supplier_Name_Normalized = df.Supplier_Name_Normalized.apply(lambda x: cleanco(x).clean_name() if type(x)==str else x)\n",
    "# Specific Polish to companies\n",
    "df.Supplier_Name_Normalized = df.Supplier_Name_Normalized.str.replace('SP ZOO', '')\n",
    "# Remove European country names from supplier names\n",
    "countries = ['ALBANIA', 'ANDORRA', 'AUSTRIA', 'AZERBAIJAN', 'BELARUS', 'BELGIUM', 'BOSNIA AND HERZEGOVINA', 'BULGARIA', 'CROATIA', 'CYPRUS', 'CZECH REPUBLIC', 'DENMARK', 'ESTONIA', 'FAROE ISLANDS', 'FINLAND', 'FRANCE', 'BRITTANY', 'GERMANY', 'DEUTSCHLAND', 'GREECE', 'GUERNSEY (CHANNEL ISLANDS)', 'HUNGARY', 'ICELAND', 'IRELAND', 'ISLE OF MAN', 'ITALY', 'JERSEY (CHANNEL ISLANDS)', 'LATVIA', 'LIECHTENSTEIN', 'LITHUANIA', 'LUXEMBOURG', 'MACEDONIA', 'MALTA', 'MOLDOVA', 'MONACO', 'MONTENEGRO', 'NETHERLANDS', 'NEDERLAND', 'HOLLAND', 'NORWAY', 'POLAND', 'POLSKA', 'PORTUGAL', 'ROMANIA', 'RUSSIA', 'SAN MARINO', 'SERBIA', 'SLOVAKIA', 'SLOVENIA', 'SPAIN', 'SWEDEN', 'SWITZERLAND', 'TURKEY', 'UNITED KINGDOM', 'UK', 'SCOTLAND', 'WALES', 'CORNWALL', 'NORTHERN IRELAND', 'UKRAINE', 'VATICAN CITY']\n",
    "for country in countries:\n",
    "    df.Supplier_Name_Normalized = df.Supplier_Name_Normalized.apply(lambda x: x.replace(country, '') if (type(x)==str and x.endswith(country)) else x)\n",
    "# Count unique values\n",
    "print 'Supplier names:', df.Supplier_Name.nunique()\n",
    "print 'Normalized names:', df.Supplier_Name_Normalized.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Converting NAICS to NIGP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crosswalk = pd.read_excel('../NAICS-CROSSWALK.xlsx')\n",
    "\n",
    "crosswalk['NAICS'] = crosswalk['NAICS'].astype(str)\n",
    "crosswalk['NIGP_CODE'] = crosswalk['NIGP_CODE'].astype(str)\n",
    "crosswalk['New NAICS'] = crosswalk['NAICS']\n",
    "crosswalk = crosswalk[['New NAICS','NIGP_CODE','NIGP_Description']]\n",
    "\n",
    "blank_work_with_converted_nigp = blank_work_with_naics.merge(crosswalk,on='New NAICS',how='left')\n",
    "\n",
    "#Reducing length of NIGP to its 3 and 5 digit version so it can be merged on our file of specific codes to care about\n",
    "successful_conversions['NIGP 3'] = successful_conversions['NIGP_CODE'].apply(lambda x: x[:3])\n",
    "successful_conversions['NIGP 5'] = successful_conversions['NIGP_CODE'].apply(lambda x: x[:5])\n",
    "\n",
    "three = pd.read_csv('../three.csv')\n",
    "five = pd.read_csv('../five.csv')\n",
    "\n",
    "three.rename(columns={'NIGP':'NIGP 3','Work Category':'Merged Work 3'},inplace=True)\n",
    "five.rename(columns={'NIGP':'NIGP 5','Work Category':'Merged Work 5'},inplace=True)\n",
    "\n",
    "three['NIGP 3']=three['NIGP 3'].astype(str)\n",
    "five['NIGP 5']=five['NIGP 5'].astype(str)\n",
    "\n",
    "successful_conversions = successful_conversions.merge(three,on='NIGP 3',how='left')\n",
    "successful_conversions = successful_conversions.merge(five,on='NIGP 5',how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import multiple files directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#https://stackoverflow.com/questions/20906474/import-multiple-csv-files-into-pandas-and-concatenate-into-one-dataframe\n",
    "    \n",
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "path = r'C:\\Users\\Griffin Strong\\OneDrive\\Omar\\My Notebooks\\NCDOT\\data\\190416\\FY 2014 Disparity Study Files\\Invoice Data\\Invoice 1' # use your path\n",
    "all_files = glob.glob(path + \"/*.xlsx\")\n",
    "\n",
    "li = []\n",
    "\n",
    "for filename in all_files:\n",
    "    df = pd.read_excel(filename, index_col=None, header=0)\n",
    "    li.append(df)\n",
    "\n",
    "invoice_1 = pd.concat(li, axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exporting Excel Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "writer = pd.ExcelWriter('../data/interim/revised_po_with_correct_amounts.xlsx', engine='xlsxwriter')\n",
    "df.to_excel(writer, sheet_name='Sheet1',index=False)\n",
    "writer.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data frame value counter frequency counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def valueCounter(df,path):\n",
    "    f= open(path,\"w+\")\n",
    "    for i in range(df.shape[1]):\n",
    "        colname = df.columns[i]\n",
    "        f.write('Column Name: '+colname+'\\n')\n",
    "        f.write('Number of Values: ' + str(len(df[colname]))+'\\n')\n",
    "        f.write('Number of Unique Values: ' + str(len(df[colname].unique()))+'\\n')\n",
    "        f.write('-----------------------------------------------------------'+'\\n')\n",
    "        f.write(str(df.iloc[:,i].value_counts(dropna=False))+'\\n')\n",
    "        f.write('\\n\\n')\n",
    "        \n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exclusion Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#County Names, State Names, City Names, Federal Agencies\n",
    "exclusion_list = [\n",
    "'Chattanooga',\n",
    "'Tennessee',\n",
    "'TN',\n",
    "'City',\n",
    "'County',\n",
    "'Sheriff', \n",
    "'Department',\n",
    "'Fire',\n",
    "'Country',\n",
    "'States',\n",
    "'State',\n",
    "'United',\n",
    "'U.S.',\n",
    "'Federal',\n",
    "'Council',\n",
    "'Government',\n",
    "'Govt.',\n",
    "'Church',\n",
    "'Ministries',\n",
    "'Ministry',\n",
    "'Theaters',\n",
    "'Theater',\n",
    "'Clubs',\n",
    "'Foundations',\n",
    "'Foundation',\n",
    "'Publishing',\n",
    "'Institute',\n",
    "'Association',\n",
    "'League',\n",
    "'Libraries',\n",
    "'Library',\n",
    "'Scouts',\n",
    "'Scout',\n",
    "'School',\n",
    "'Daycare',\n",
    "'Learning',\n",
    "'Chapter',\n",
    "'Commissions',\n",
    "'Commission',\n",
    "'YMCA',\n",
    "'YWCA',\n",
    "'College',\n",
    "'University',\n",
    "'Society',\n",
    "'Chamber',\n",
    "'Hospitals',\n",
    "'Hospital',\n",
    "'Bureau',\n",
    "'Department',\n",
    "'Agency',\n",
    "'National',\n",
    "'Goodwill',\n",
    "'Salvation',\n",
    "'Unions',\n",
    "'Union',\n",
    "'Catholic',\n",
    "'Baptist',\n",
    "'Methodist',\n",
    "'Muslim',\n",
    "'Jewish',\n",
    "'Presbyterian',\n",
    "'Episcopal',\n",
    "'Junior',\n",
    "'Circuit',\n",
    "'Court',\n",
    "'Alliance',\n",
    "'District',\n",
    "'A.S.S.N',\n",
    "'Habitat',\n",
    "'Humanity',\n",
    "'Non-profit',\n",
    "'Organization',\n",
    "'Authority',\n",
    "'Center',\n",
    "'Development',\n",
    "'Campaign',\n",
    "'Conference',\n",
    "'Board',\n",
    "'Division',\n",
    "'Awareness',\n",
    "'Christian',\n",
    "'Museum',\n",
    "'Charity',\n",
    "'Health Care',\n",
    "'Health',\n",
    "'Dept',\n",
    "'Coalition', \n",
    "'Collaborative',\n",
    "'Books',\n",
    "'Magazine',\n",
    "'Grants',\n",
    "'Grant',\n",
    "'Lease-property',\n",
    "'Wages',\n",
    "'Land',\n",
    "'Reimbursements',\n",
    "'Reimbursement',\n",
    "'Expenses',\n",
    "'Expense',\n",
    "'Utilities',\n",
    "'Utility',\n",
    "'Trust',\n",
    "'Park',\n",
    "'Natl',\n",
    "'Committee',\n",
    "'Sports',\n",
    "'Sport',\n",
    "'Habitat',\n",
    "'Centers',\n",
    "'Treasury',\n",
    "'Dep',\n",
    "'United',\n",
    "'Govrnt',\n",
    "'Gov',\n",
    "'Govt',\n",
    "'YMCA',\n",
    "'Community',\n",
    "'Assn',\n",
    "'Univ',\n",
    "'Municipal',\n",
    "'Police',\n",
    "'Assoc',\n",
    "'Cross',\n",
    "'Public',\n",
    "'Safety',\n",
    "'Agncy',\n",
    "'Enforcement',\n",
    "'Bank',\n",
    "'Children',\n",
    "'Partnership',\n",
    "'Nursery',\n",
    "'Depart',\n",
    "'Postal'\n",
    "]\n",
    "\n",
    "def mfd_exclusions(df,name_column):\n",
    "    df['Potentially Exclude'] = False\n",
    "    for index, row in df.iterrows(): \n",
    "        array = df.iloc[index][name_column].title().split()\n",
    "        #print(array)\n",
    "        for i in range(len(array)):\n",
    "            #print(i)\n",
    "            if array[i] in exclusion_list:\n",
    "                #print(i)\n",
    "                df.at[index,'Potentially Exclude'] = True\n",
    "                \n",
    "mfd_exclusions(master,'Business Name')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# String Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def extract_city(string):\n",
    "    array = str(string).split(',')\n",
    "    return array[0]\n",
    "\n",
    "tnucp['Real City'] = tnucp['City'].apply(extract_city)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exporting CSV Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "mvf.to_csv('path',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reduce columns, Delete columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "fy13 = fy13[['col1','col2']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "df.drop(['B', 'C'], axis=1) #Deletes column B and C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find intersection of two dataframe's column name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "set(po_item.columns).intersection(set(po_header.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resolve Multiple Addresses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hub.loc[hub['Address 1'].notnull()&hub['Address 2'].isnull(),'Address'] = hub['Address 1']\n",
    "hub.loc[hub['Address 1'].notnull()&hub['Address 2'].notnull(),'Address'] = hub['Address 1'].astype(str) + ' ' + hub['Address 2'].astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Print list of columns fill when align aligning compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.setdiff1d(['MFD','Source', \n",
    "       'Active Vendor Flag','Vendor Number',  \n",
    "       'Business Name', 'Smoothed Name', 'Owner', 'Phone', 'Email', 'Address',\n",
    "       'City', 'County', 'State', 'Zip', 'DBE Category', 'Work Description',\n",
    "       'NIGP 5 Vendor', 'NIGP 3 Vendor', 'All Vendor NIGP', 'Ethnicity',\n",
    "        'Vendor Work Categorization Type',\n",
    "       'Potential Minority Identifier', 'Relevant Market Region',\n",
    "       'Potentially Exclude', 'Actually Exclude'],hub.columns)\n",
    "\n",
    "for i in array:\n",
    "    print('df[\\''+i+'\\'] = np.nan')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mark Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mark_duplicates(df,column_list):\n",
    "    df['Duplicate']=df.duplicated(subset=column_list)\n",
    "    dup_criteria = ''\n",
    "    for string in column_list:\n",
    "        dup_criteria = dup_criteria+string+', '\n",
    "    df.loc[df['Duplicate']==True,'MFD'] = 'Duplicate by ' + dup_criteria\n",
    "    df.drop(['Duplicate'],axis=1,inplace=True)\n",
    "    return df\n",
    "\n",
    "master=mark_duplicates(df,['Business Name','Email','County'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mark_duplicates(df,column_list):\n",
    "    df['Duplicate']=df.duplicated(subset=column_list)\n",
    "    df.loc[df['Duplicate']==True,'MFD'] = 'Duplicate by PO_NBR'\n",
    "    exclusions = df[df['MFD'].notnull()].copy()\n",
    "    df = df[df['MFD'].isnull()].copy()\n",
    "    \n",
    "    return df, exclusions\n",
    "\n",
    "priority_frame,exclusions_1=mark_duplicates(priority_frame,['PO_NBR'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting the frequency count number of times a value appears in a data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "temp = DF_GOES_HERE.drop_duplicates(subset=['Cleaned Name','Hub Certification'])\n",
    "def createDupNameFrame(df,name_column):\n",
    "    name_counts = df[name_column].value_counts().rename('name_counts')\n",
    "\n",
    "    dup_names = df.merge(name_counts.to_frame(),\n",
    "                                    left_on=name_column,\n",
    "                                    right_index=True)\n",
    "\n",
    "    dup_names = dup_names[dup_names['name_counts']>1]\n",
    "    return dup_names\n",
    "\n",
    "dup_names = createDupNameFrame(temp,'NAME COLUMN GOES HERE')\n",
    "pd.set_option('display.max_rows',None)\n",
    "dup_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def createDupNameFrame(df,name_column):\n",
    "    name_counts = df[name_column].value_counts().rename('Number of Instances')\n",
    "\n",
    "    dup_names = df.merge(name_counts.to_frame(),\n",
    "                                    left_on=name_column,\n",
    "                                    right_index=True)\n",
    "    dup_names['Number of Instances Key'] = name_column\n",
    "#     dup_names = dup_names[dup_names['name_counts']>1]\n",
    "    return dup_names\n",
    "\n",
    "DF_GOES_HERE = createDupNameFrame(DF_GOES_HERE,'NAME COLUMN GOES HERE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "name_counts = mwbe_no_null_eth['Po Supplier Name'].value_counts().rename('name_counts')\n",
    "\n",
    "ethnicity_conflicts = mwbe_no_null_eth.merge(name_counts.to_frame(),\n",
    "                                left_on='Po Supplier Name',\n",
    "                                right_index=True)\n",
    "\n",
    "ethnicity_conflicts = ethnicity_conflicts[ethnicity_conflicts['name_counts']>1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regular Expression Req Number regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_req(req):\n",
    "    x = re.findall('([\\d]{5})', req)\n",
    "    if not x:\n",
    "        return np.nan\n",
    "    else:\n",
    "        string = ';'.join(x)\n",
    "        return(string)\n",
    "\n",
    "master['Cleaned Req Number 2'] = master['Cleaned Req Number'].astype(str).apply(clean_req)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Return a two 2 values dataframes from a function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "merge_results,po_iteration=ethnicity_pull(po,mwbe,'Po Supplier Name','Cleaned Name',False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set column as string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "master['Email'] = master['Email'].str\n",
    "df['Item Category'] =df['Item Category'].astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove replace substring from string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "y = x.replace('.good','')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "df.insert(0,'MFD','')\n",
    "df['NIGP 5'] = ''\n",
    "df['NIGP 3'] = ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Insert Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "chat.insert(0,'Source','Source File Name')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Slice shorten strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "df['NIGP 5'] = df['NIGP'].str.slice(0,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VLOOKUP, merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "results_fy13 = fy13.merge(three,on='NIGP 3',how='left')\n",
    "# Note, three is the table we are looking up values on, fy13 is the main\n",
    "# table, and NIGP 3 is the shared column nam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove dash, commas, character from string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "df['NIGP 7'] = df['NIGP 7'].str.replace(\"-\",\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Display entire cell content full non-truncated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concatenating data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "master = pd.concat([df1,df2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove Scientific, Globally setting the float length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "pd.options.display.float_format = '{:.5f}'.format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Changing default number of rows all columns displayed show all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting values from comma separated cells new line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "tnucp.rename(columns={0:'Col0',1:'Col1',2:'Col2',3:'Col3',4:'Col4',5:'Col5'},inplace=True)\n",
    "\n",
    "def clean_firm_name(df):\n",
    "    for index, row in df.iterrows():\n",
    "        array = str(df.iloc[index]['Company Information ']).split('\\n')\n",
    "        for i in range(len(array)):\n",
    "            df.at[index,i] = array[i]\n",
    "            \n",
    "    #New addition, may break it, need to expand this function to be able to take this    \n",
    "    tnucp.rename(columns={0:'Col0',1:'Col1',2:'Col2',3:'Col3',4:'Col4',5:'Col5'},inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find duplicated rows "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#Note you want to check that there are now Emails/Phones with blanks because those can be considered duplicates\n",
    "#master_women[master_women['Email'].apply(len) < 4]\n",
    "#master_women[master_women['Phone'].apply(len) < 10]\n",
    "master_women[master_women.duplicated(subset=['Business Name','Email','Phone'])]\n",
    "\n",
    "#Afterwards manually sift through the duplicated rows\n",
    "master_women[master_women['Email'].duplicated()].sort_values(by=['Email'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Drop Null NA by specified column with NA values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "df = df.dropna(subset=['NIGP'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "mvf = mvf[mvf['Fixed Ethnicity'].notnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rename column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "df.rename(columns={'Old String':'New String'},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#Fix for the sitatuion where merging causes duplicate rows\n",
    "#I believe this is caused by a scenario where you are trying to merge with a table that\n",
    "#Is not explicitly defined to be a string, so you have to explicitly define it as this\n",
    "df1['Cost Center Code'] = df1['Cost Center Code'].astype(str)\n",
    "df1['Cost Center Name'] = df1['Cost Center Name'].astype(str)\n",
    "df2['Department Code'] = df2['Department Code'].astype(str)\n",
    "df2['Department Name'] = df2['Department Name'].astype(str)\n",
    "#Then do merge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iterate through dataframe and apply unique function to modify each value in a different way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def mfd_zip(df):\n",
    "    for index, row in df.iterrows():\n",
    "        if(df.iloc[index]['Zip'][0].isalpha()):\n",
    "            df.at[index,'MFD'] = 'Out of Country Zip'\n",
    "        elif(df.iloc[index]['Zip'] == '00000'):\n",
    "            df.at[index,'MFD'] = 'Invalid Zip Code'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assigning booleans based on a column, checking if string is a number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "df['Is NIGP'] = df['NIGP'].str.isdigit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "Check if string is positive digit (integer) and alphabet\n",
    "You may use str.isdigit() and str.isalpha() to check whether given string is positive integer and alphabet respectively.\n",
    "\n",
    "Sample Results:\n",
    "\n",
    "# For alphabet\n",
    ">>> 'A'.isdigit()\n",
    "False\n",
    ">>> 'A'.isalpha()\n",
    "True\n",
    "\n",
    "# For digit\n",
    ">>> '1'.isdigit()\n",
    "True\n",
    ">>> '1'.isalpha()\n",
    "False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Marking Data Gaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hard exclusions\n",
    "#df['Award Date']=pd.to_datetime(df['Award Date'],infer_datetime_format=True) \n",
    "df.loc[(~(df['Award Date']>='1-1-14')|~(df['Award Date']<='12-31-18'))&(df['MFD']).isnull(),'MFD'] = 'Outside Date Range'\n",
    "df.loc[((df['Award Amount'].isnull())|(df['Award Amount']==0))&(df['MFD'].isnull()),'MFD'] = 'Null Award Amount'\n",
    "df.loc[(df['Award Amount']<1000)&(df['MFD']).isnull(),'MFD'] = 'Less than 1000'\n",
    "\n",
    "# Creating a function to mark exclusions where multiple individual items can be placed in the MFD column The goal should be to preserve\n",
    "# what is already in MFD and to exclude rows that have much deeper reasons for exclusions such as being exclusion categories or less than threshold map\n",
    "def mark_gaps(df):\n",
    "    # Set a boolean so that we can maintain the integrity of not overwriting those rows that have deeper exclusion reasons \n",
    "    # (hence the benefit of the MFD.isnull check in the loc statements)\n",
    "    df.loc[df['MFD'].notnull(), 'Hard Exclusion'] = 'Y'\n",
    "    # Make all null values empty strings\n",
    "    df['MFD'].replace(np.nan, '', inplace=True)\n",
    "    # Marking Gaps\n",
    "    df.loc[(df['Zip'].isnull()) &\n",
    "           (df['Hard Exclusion'].isnull()), 'MFD'] = df['MFD'] + 'No Zip,'\n",
    "    df.loc[(df['Work Category'].isnull()) &\n",
    "           (df['Hard Exclusion'].isnull()), 'MFD'] = df['MFD'] + 'No WC,'\n",
    "    df.loc[(df['Work Category'] == '?') &\n",
    "           (df['Hard Exclusion'].isnull()), 'MFD'] = df['MFD'] + 'No WC,'\n",
    "    # Fixing the good rows so they have null values in the MFD again\n",
    "    df['MFD'].replace('',np.nan ,inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Converting Force forcing multiple rows to a single row and preserving unique data condense consolidate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def force_single_row(df,force_column,primary_key):\n",
    "    df[force_column] = df[force_column].astype(str)\n",
    "    joined_phones=df.groupby(primary_key)[force_column].apply(';'.join).reset_index()\n",
    "    joined_phones.rename(columns={force_column:'All '+force_column+'s'},inplace=True)\n",
    "    df = df.merge(joined_phones,on=primary_key,how='left')\n",
    "    df.drop_duplicates(subset=[primary_key],inplace=True)\n",
    "    return df\n",
    "\n",
    "email_master = force_single_row(email_master,'Email','VENDOR_NBR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "construction['Phone'] = construction['Phone'].astype(str)\n",
    "joined_phones=construction.groupby('Company Name')['Phone'].apply(';'.join).reset_index()\n",
    "joined_phones.rename(columns={'Phone':'All Phones'},inplace=True)\n",
    "construction = construction.merge(joined_phones,on='Company Name',how='left')\n",
    "construction.drop_duplicates(subset='Company Name',inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filtering so only rows with true in column remain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "df = df[df['Is NIGP']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert NAICS to NIGP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def naics_to_nigp(master,path):\n",
    "    crosswalk = pd.read_excel(path)\n",
    "    crosswalk['NAICS'] = crosswalk['NAICS'].astype(str)\n",
    "    crosswalk['NIGP 5'] = crosswalk['NIGP_CODE'].astype(str)\n",
    "    crosswalk['NIGP 3'] = crosswalk['NIGP 5'].astype(str).apply(lambda x: x[:3])\n",
    "    crosswalk = crosswalk[['NAICS','NIGP 5','NIGP 3','NIGP_Description']]\n",
    "    #There are multiple NIGP codes for a single NAICS code but I have already verified that it is arbitrary.\n",
    "    crosswalk.drop_duplicates(subset=['NAICS'],inplace=True)\n",
    "\n",
    "    def fix_nigp_5(string):\n",
    "        string = str(string)\n",
    "        if(len(string)==4):\n",
    "            return '0'+string\n",
    "        elif(len(string)==3):\n",
    "            return '00'+string\n",
    "        else:\n",
    "            return string\n",
    "    \n",
    "    crosswalk['NIGP 5'] = crosswalk['NIGP 5'].apply(fix_nigp_5)\n",
    "    master = master.merge(crosswalk,on='NAICS',how='left')\n",
    "    return master\n",
    "\n",
    "master = naics_to_nigp(master,'../NAICS-CROSSWALK.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert NIGP to Work Category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function requires the master frame to have an NIGP 3 and NIGP 5 column\n",
    "def nigp_to_work_category(master,path):\n",
    "    nigp = pd.read_excel(path)\n",
    "    # Making decisions as to how to classify certain codes\n",
    "    nigp.loc[nigp['Work Category']=='Non-Commercial Activity','Work Category'] = 'Goods & Supplies'\n",
    "    nigp.loc[nigp['Work Category']=='Non-Commerical Transactions','Work Category'] = 'Other Services'\n",
    "    nigp.loc[nigp['Work Category']=='Non-Professional Services','Work Category'] = 'Other Services'\n",
    "    \n",
    "    # Fixing NIGP Codes so that they can merge\n",
    "    def fix_nigp_3(string):\n",
    "        string=str(string)\n",
    "        if(len(string)==1):\n",
    "            return '00'+string\n",
    "        elif(len(string)==2):\n",
    "            return '0'+string\n",
    "        else:\n",
    "            return string\n",
    "    \n",
    "    def fix_nigp_5(string):\n",
    "        string = str(string)\n",
    "        if(len(string)==4):\n",
    "            return '0'+string\n",
    "        elif(len(string)==3):\n",
    "            return '00'+string\n",
    "        elif(len(string)==2):\n",
    "            return '000'+string\n",
    "        else:\n",
    "            return string\n",
    "    \n",
    "    nigp['NIGP 3'] = nigp['NIGP 3'].apply(fix_nigp_3)\n",
    "    nigp['NIGP 5'] = nigp['NIGP 5'].apply(fix_nigp_5)\n",
    "    \n",
    "    nigp_3 = nigp[['NIGP 3','Commodity Description','Work Category']].copy()\n",
    "    nigp_5 = nigp[['NIGP 5','Commodity Description','Work Category']].copy()\n",
    "    \n",
    "    nigp_3.rename(columns={'Commodity Description':'Commodity Description 3','Work Category':'Work Category 3'},inplace=True)\n",
    "    nigp_5.rename(columns={'Commodity Description':'Commodity Description 5','Work Category':'Work Category 5'},inplace=True)\n",
    "    \n",
    "    master = master.merge(nigp_3,on='NIGP 3',how='left')\n",
    "    master = master.merge(nigp_5,on='NIGP 5',how='left')\n",
    "    \n",
    "    # Filling work final work category in order of most significant matches to least\n",
    "    master['Final Work Category'] = master['Work Category 5']\n",
    "    master['Final Work Category'].fillna(master['Work Category 3'], inplace=True)\n",
    "    \n",
    "    return master\n",
    "\n",
    "master = nigp_to_work_category(master,'../NIGP to Work Category from Rom - Cleaned - 190725ob.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Relevant Market"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating Unique Arrays for the Relevant Market Regions\n",
    "Main_County = {\n",
    "    'Hamilton County':'TN'\n",
    "}\n",
    "MSA_Counties = {\n",
    "    'Catoosa County':'GA',\n",
    "    'Dade County':'GA',\n",
    "    'Marion County':'TN',\n",
    "    'Sequatchie County':'TN',\n",
    "    'Walker County':'GA'\n",
    "}\n",
    "CSA_Counties = {\n",
    "    'Bradley County':'TN',\n",
    "    'Jackson County':'AL',\n",
    "    'Mcminn County':'TN',\n",
    "    'Murray County':'GA',\n",
    "    'Polk County':'TN',\n",
    "    'Rhea County':'TN',\n",
    "    'Whitfield County':'GA'\n",
    "}\n",
    "Surrounding_Counties = {\n",
    "    'Bledsoe Couny':'TN',\n",
    "    'Meigs County':'TN'\n",
    "}\n",
    "Main_State = 'TN'\n",
    "Relevant_States = ['TN','GA','KY','VA','NC','AL','MS','MO','AR']\n",
    "Relevant_Market_Order = ['Main County','MSA','CSA','Surrounding Counties','TN','GA','KY','VA','NC','AL','MS','MO','AR', 'USA']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function checks if the value of the passed dictionary matches the passed state\n",
    "#For this to work, the dictionary needs to be inverted because searching a dictionary\n",
    "#only works on keys, so the key value pair needs to be flipped\n",
    "\n",
    "#This is a helper function for the create_relevant_market function\n",
    "\n",
    "def check_inverse_mapping(state,dictionary):\n",
    "    #Inverting the mapping of the dictionary so I can check both key and value in a one line if statement\n",
    "    #https://stackoverflow.com/questions/483666/python-reverse-invert-a-mapping\n",
    "    inv_map = {}\n",
    "    for k, v in dictionary.items():\n",
    "        #inv_msa[v] = inv_msa.get(v, [])\n",
    "        #inv_msa[v].append(k)\n",
    "        inv_map.setdefault(v, []).append(k)\n",
    "        \n",
    "    if state in inv_map:\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_relevant_market(df):\n",
    "    df['Relevant Market Region'] = ''\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        temp_state = df.iloc[index]['State'].strip()\n",
    "        temp_county = df.iloc[index]['County'].strip()\n",
    "        if temp_county in Main_County and temp_state == Main_County[temp_county]:\n",
    "            df.at[index,'Relevant Market Region'] = 'Main County'\n",
    "        elif temp_county in MSA_Counties and check_inverse_mapping(temp_state,MSA_Counties):\n",
    "            df.at[index,'Relevant Market Region'] = 'MSA'\n",
    "        elif temp_county in CSA_Counties and check_inverse_mapping(temp_state,CSA_Counties):\n",
    "            df.at[index,'Relevant Market Region'] = 'CSA'\n",
    "        elif temp_county in Surrounding_Counties and check_inverse_mapping(temp_state,Surrounding_Counties):\n",
    "            df.at[index,'Relevant Market Region'] = 'Surrounding Counties'\n",
    "        elif temp_state in Relevant_States:\n",
    "            df.at[index,'Relevant Market Region'] = temp_state\n",
    "        else:\n",
    "            #Else it is within the USA\n",
    "            df.at[index,'Relevant Market Region'] = 'USA' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fix Clean NIGP "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_nigp_3(string):\n",
    "    string=str(string)\n",
    "    if(len(string)==1):\n",
    "        return '00'+string\n",
    "    elif(len(string)==2):\n",
    "        return '0'+string\n",
    "    else:\n",
    "        return string\n",
    "    \n",
    "    rom_nigp['NIGP 3'] = rom_nigp['NIGP 3'].apply(fix_nigp_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_nigp_5(string):\n",
    "    string = str(string)\n",
    "    if(len(string)==4):\n",
    "        return '000'+string\n",
    "    elif(len(string)==3):\n",
    "        return '00'+string\n",
    "    else:\n",
    "        return string\n",
    "    \n",
    "rom_nigp['NIGP 5'] = rom_nigp['NIGP 5'].apply(fix_nigp_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_nigp_7(string):\n",
    "    string = str(string)\n",
    "    if(len(string)==6):\n",
    "        return '0'+string\n",
    "    elif(len(string)==5):\n",
    "        return '00'+string\n",
    "    elif(len(string)==4):\n",
    "        return '000'+string\n",
    "    elif(len(string)==3):\n",
    "        return '0000'+string\n",
    "    elif(len(string)==2):\n",
    "        return '00000'+string\n",
    "    else:\n",
    "        return string\n",
    "    \n",
    "seven_description['NIGP'] = seven_description['NIGP'].apply(fix_nigp_7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applying Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def times2(x):\n",
    "    return x*2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "df['col1'].apply(times2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "df['col3'].apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "df['col1'].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Copy a dataframe to avoid modifying a slice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#https://stackoverflow.com/questions/38835483/confusion-re-pandas-copy-of-slice-of-dataframe-warning\n",
    "chat = chat[['Source','Name','Owner','DBE Category','Office Phone','Final Ethnicity']].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distance between two strings Levenshtein Distance similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from difflib import SequenceMatcher\n",
    "\n",
    "def similar(a, b):\n",
    "    return SequenceMatcher(None, a, b).ratio()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Number of Instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getNumberOfInstances(df,name_column):\n",
    "    name_counts = df[name_column].value_counts().rename('Number of Instances')\n",
    "\n",
    "    dup_names = df.merge(name_counts.to_frame(),\n",
    "                                    left_on=name_column,\n",
    "                                    right_index=True)\n",
    "    return dup_names\n",
    "\n",
    "df = getNumberOfInstances(df,'VENDOR_NBR')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean Simple Cleaning Phone Numbers Email"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def clean_phone(string):\n",
    "    string = str(string).replace(\" \", \"\").replace(\"(\",\"\").replace(\")\",\"\").replace(\"-\",\"\").replace(\".\",\"\")\n",
    "    string = str(string)[:10]\n",
    "    \n",
    "    if(len(string)<10):\n",
    "        return np.nan\n",
    "    else:\n",
    "        return string\n",
    "\n",
    "master['Phone'] = master['Phone'].apply(clean_phone)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def clean_zip(string):\n",
    "    string = str(string)[:5]\n",
    "    \n",
    "    if(len(string)<5):\n",
    "        return np.NaN\n",
    "    else:\n",
    "        return string\n",
    "\n",
    "master['Zip'] = master['Zip'].apply(clean_zip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def clean_email(string):\n",
    "    string = str(string).lower().strip()\n",
    "    \n",
    "    if(len(string)<5):\n",
    "        return np.NaN\n",
    "    else:\n",
    "        return string\n",
    "\n",
    "master['Email'] = master['Email'].apply(clean_email)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lambda conditional if else"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "df['c']=df['b'].apply(lambda x: 0 if x ==0 else math.log(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correcting Zip Codes Syntax Print Line Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "po[po['County'].isnull()]['Zip Code'].unique()\n",
    "\n",
    "incorrect_zips = ['02129', '07647', '08066', 'R3B 0', '08107', '01730', '07960',\n",
    "       '06450', '01742', '06904', 'V8W 1', '08054', '02451', '02142',\n",
    "       '01441', '06820', '07728', '06461', '02809', '06810', '01040',\n",
    "       '02241', '44204', '06484', '03820', '01966', '02026', '05101',\n",
    "       '02109', '02038', '01373', '06010', '22767', '06492', '02145',\n",
    "       '02446', '01752', '02494', '07801', '06475', '08085', 'K1G 6',\n",
    "       '08701', '02072', '08110', '02116', '07083', '08361', '07511',\n",
    "       '01719', 'EH2 2', '02210', 'E8 4R', '08057', '08033']\n",
    "\n",
    "def zip_code_syntax_corrector(array):\n",
    "    for i in range(len(array)):\n",
    "        print('po.loc[po[\\'Zip Code\\']==\\''  +str(array[i])+'\\',\\'County\\'] = \\'\\'')\n",
    "        \n",
    "        \n",
    "zip_code_syntax_corrector(incorrect_zips)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean Cleaning Names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanest_names(df,name_column,no_new_column):\n",
    "    df['Cleanest Name'] = df[name_column]\n",
    "    df['Cleanest Name'] = df['Cleanest Name'].apply(lambda x: str(x).replace(\"&\",''))\n",
    "    df['Cleanest Name'] = df['Cleanest Name'].apply(lambda x: str(x).replace(\"-\",''))\n",
    "    df['Cleanest Name'] = df['Cleanest Name'].apply(lambda x: str(x).replace(',',''))\n",
    "    df['Cleanest Name'] = df['Cleanest Name'].apply(lambda x: str(x).replace('.',''))\n",
    "    df['Cleanest Name'] = df['Cleanest Name'].apply(lambda x: str(x).replace(\"'\",''))\n",
    "    df['Cleanest Name'] = df['Cleanest Name'].apply(lambda x: str(x).upper())\n",
    "    df['Cleanest Name'] = df['Cleanest Name'].apply(lambda x: str(x).lstrip())\n",
    "    df['Cleanest Name'] = df['Cleanest Name'].apply(lambda x: str(x).rstrip())\n",
    "    df['Cleanest Name'] = df['Cleanest Name'].apply(lambda x: str(x)[:-4] if x.endswith('llc') else x)\n",
    "    df['Cleanest Name'] = df['Cleanest Name'].apply(lambda x: str(x)[:-5] if x.endswith('pllc') else x)\n",
    "    df['Cleanest Name'] = df['Cleanest Name'].apply(lambda x: str(x)[:-4] if x.endswith('inc') else x)\n",
    "    df['Cleanest Name'] = df['Cleanest Name'].apply(lambda x: str(x)[:-4] if x.endswith('ltd') else x)\n",
    "    df['Cleanest Name'] = df['Cleanest Name'].apply(lambda x: str(x)[:-5] if x.endswith('corp') else x)\n",
    "    df['Cleanest Name'] = df['Cleanest Name'].apply(lambda x: str(x).replace(\" \",''))\n",
    "    \n",
    "    if(no_new_column):\n",
    "        df[name_column] = df['Cleanest Name']\n",
    "        df.drop(['Cleanest Name'], axis=1,inplace=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def clean_names(df,name_column,no_new_column):\n",
    "    df['Cleaned Name'] = df[name_column]\n",
    "    df['Cleaned Name'] = df['Cleaned Name'].apply(lambda x: str(x).replace(',',''))\n",
    "    df['Cleaned Name'] = df['Cleaned Name'].apply(lambda x: str(x).replace('.',''))\n",
    "    df['Cleaned Name'] = df['Cleaned Name'].apply(lambda x: str(x).replace(\"'\",''))\n",
    "    df['Cleaned Name'] = df['Cleaned Name'].apply(lambda x: str(x).title())\n",
    "    df['Cleaned Name'] = df['Cleaned Name'].apply(lambda x: str(x).lstrip())\n",
    "    df['Cleaned Name'] = df['Cleaned Name'].apply(lambda x: str(x).rstrip())\n",
    "    df['Cleaned Name'] = df['Cleaned Name'].apply(lambda x: str(x)[:-4] if x.endswith('Llc') else x)\n",
    "    df['Cleaned Name'] = df['Cleaned Name'].apply(lambda x: str(x)[:-5] if x.endswith('Pllc') else x)\n",
    "    df['Cleaned Name'] = df['Cleaned Name'].apply(lambda x: str(x)[:-4] if x.endswith('Inc') else x)\n",
    "    df['Cleaned Name'] = df['Cleaned Name'].apply(lambda x: str(x)[:-4] if x.endswith('Ltd') else x)\n",
    "    df['Cleaned Name'] = df['Cleaned Name'].apply(lambda x: str(x)[:-5] if x.endswith('Corp') else x)\n",
    "    \n",
    "    if(no_new_column):\n",
    "        df[name_column] = df['Cleaned Name']\n",
    "        df.drop(['Cleaned Name'], axis=1,inplace=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Award File Columns needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Awards + Vendors\n",
    "df = df[['Number of Instances', 'MFD', 'Source', 'Pulled From', 'ACTIVE_FLAG','Business Name',\n",
    "       'Smoothed Name', 'Owner', 'Phone', 'Email', 'Address',\n",
    "       'City', 'County', 'State', 'Zip', 'DBE Category', 'Work Description',\n",
    "       'NIGP 5', 'NIGP 3','Work Category',\n",
    "       'Work Categorization Type', 'Potential Minority Identifier',\n",
    "       'Relevant Market Region',\n",
    "        'Po Number', 'Po Creation Date',\n",
    "       'Status',\n",
    "        'Po Item Description', \n",
    "       'Po Comments', 'Item Category',\n",
    "       'Small Business Flag', 'Women Owned Flag', \n",
    "       'Actual Shipment Amount', 'Final Work Category',\n",
    "       'Potentially Exclude',\n",
    "       'Null Shipment Amount', \n",
    "       'Department Name', 'Ethnicity', 'Certification']].copy()\n",
    "\n",
    "# AWARDS\n",
    "po = po[['Source', 'MFD', 'Po Number', 'Po Creation Date',\n",
    "       'Status', 'Po Supplier Name',\n",
    "       'Zip', 'Po Item Description', \n",
    "       'Po Comments', 'Item Category',\n",
    "       'Small Business Flag', 'Women Owned Flag', \n",
    "       'NIGP 7', 'NIGP 5', 'NIGP 3', 'Commodity Description 3',\n",
    "       'Work Category 3', 'Commodity Description 5', 'Work Category 5',\n",
    "       '7 Digit Lookup', 'Actual Shipment Amount', 'Final Work Category',\n",
    "        'City', 'County', 'State', 'Potentially Exclude',\n",
    "       'Null Shipment Amount', \n",
    "       'Department Name', 'Ethnicity', 'Certification', 'Relevant Market Region']]\n",
    "\n",
    "\n",
    "# VENDORS\n",
    "master = master[['Number of Instances','ACTIVE_FLAG', 'MFD', 'Source', 'Pulled From', 'Business Name',\n",
    "       'Smoothed Name', 'VENDOR_NBR', 'Owner', 'Phone', 'Email', 'Address',\n",
    "       'City', 'County', 'State', 'Zip', 'DBE Category', 'Work Description',\n",
    "       'NIGP 5', 'NIGP 3', 'Ethnicity', 'Certification', 'Work Category',\n",
    "       'Work Categorization Type', 'Potential Minority Identifier',\n",
    "       'Relevant Market Region', 'Potentially Exclude']].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Expand Explode semi colon separated values into new rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def explode_naics(df):\n",
    "    s = df[\"Extracted NAICS\"].str.split(',', expand=True).stack()\n",
    "    i = s.index.get_level_values(0)\n",
    "    df2 = df.loc[i].copy()\n",
    "    df2[\"New NAICS\"] = s.values\n",
    "    \n",
    "    #Strip is important here because it removes white spaces\n",
    "    df2['New NAICS'] = df2['New NAICS'].apply(lambda x: x.strip())\n",
    "    return df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def explode_email(df):\n",
    "    s = df[\"Email Address\"].str.split(';', expand=True).stack()\n",
    "    i = s.index.get_level_values(0)\n",
    "    df2 = df.loc[i].copy()\n",
    "    df2[\"Email Address\"] = s.values\n",
    "    return df2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create horizontal bar create meta table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createMetaTable(df):\n",
    "    col_names = []\n",
    "    col_amount = []\n",
    "    col_unique = []\n",
    "\n",
    "    for col in df.columns: \n",
    "        col_names.append(col) \n",
    "        \n",
    "        col_name = str(col)\n",
    "        temp = df.dropna(subset=[col_name])\n",
    "        col_amount.append(len(temp[col_name]))\n",
    "        \n",
    "        col_unique.append(len(temp[col_name].unique()))\n",
    "\n",
    "    # intialise data of lists. \n",
    "    data = {'Column Name':col_names, 'Fill Amount':col_amount,'Number Unique':col_unique} \n",
    "\n",
    "    # Create DataFrame \n",
    "    df = pd.DataFrame(data)\n",
    "    df.sort_values(by=['Fill Amount','Number Unique'],ascending=False,inplace=True)\n",
    "    \n",
    "    # Marking columns with less than 5 percent of the max number of rows\n",
    "    maxValue = df['Fill Amount'].max()\n",
    "    df.loc[df['Fill Amount']<(0.05*maxValue),'Less than 5 Percent'] = 'Y'\n",
    "    \n",
    "    return df\n",
    "\n",
    "def createLayeredBar(meta,path):\n",
    "    import altair as alt\n",
    "    \n",
    "    # Ref: https://altair-viz.github.io/user_guide/generated/core/altair.EncodingSortField.html\n",
    "    # Ref: https://altair-viz.github.io/user_guide/encoding.html#sorting-legends\n",
    "    # Ref: https://vega.github.io/vega/docs/schemes/\n",
    "    \n",
    "    meta_fill = meta[['Column Name','Fill Amount']].copy()\n",
    "    meta_fill.rename(columns={'Fill Amount':'Amount'},inplace=True)\n",
    "    meta_unique = meta[['Column Name','Number Unique']].copy()\n",
    "    meta_unique.rename(columns={'Number Unique':'Amount'},inplace=True)\n",
    "    meta_fill['Type'] = 'Fill Amount'\n",
    "    meta_unique['Type'] = 'Number Unique'\n",
    "    meta = pd.concat([meta_fill,meta_unique])\n",
    "\n",
    "    alt.renderers.enable('notebook')\n",
    "\n",
    "    chart = alt.Chart(meta).mark_bar(opacity=0.7).encode(\n",
    "        x=alt.X('Amount:Q',stack=None),\n",
    "        y=alt.Y('Column Name:O',sort=alt.EncodingSortField(field='Amount',op='count',order='ascending')),\n",
    "        #color=\"Type\",\n",
    "        color=alt.Color('Type', scale=alt.Scale(scheme='dark2')),\n",
    "        tooltip=['Column Name', 'Type', 'Amount']\n",
    "    ).interactive()\n",
    "    \n",
    "    chart.save(path)\n",
    "    \n",
    "    return chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-9b29e207e188>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m \u001b[0mmeta\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreateColumnGraph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'exports/190926/ContractQuality - Data Gaps.png'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "%matplotlib inline \n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def createMetaTable(df):\n",
    "    col_names = []\n",
    "    col_amount = []\n",
    "    col_unique = []\n",
    "\n",
    "    for col in df.columns: \n",
    "        col_names.append(col) \n",
    "        \n",
    "        col_name = str(col)\n",
    "        temp = df.dropna(subset=[col_name])\n",
    "        col_amount.append(len(temp[col_name]))\n",
    "        \n",
    "        col_unique.append(len(temp[col_name].unique()))\n",
    "\n",
    "    # intialise data of lists. \n",
    "    data = {'Column Name':col_names, 'Fill Amount':col_amount,'Number Unique':col_unique} \n",
    "\n",
    "    # Create DataFrame \n",
    "    df = pd.DataFrame(data)\n",
    "    df.sort_values(by=['Fill Amount','Number Unique'],ascending=False,inplace=True)\n",
    "    \n",
    "    # Marking columns with less than 5 percent of the max number of rows\n",
    "    maxValue = df['Fill Amount'].max()\n",
    "    df.loc[df['Fill Amount']<(0.05*maxValue),'Less than 5 Percent'] = 'Y'\n",
    "    \n",
    "    return df\n",
    "\n",
    "def createHorizontalBar(df,x_col,y_col,title,path,filter_gapless):\n",
    "    maxValue = df[x_col].max()\n",
    "    \n",
    "    if(filter_gapless==True):\n",
    "        df = df[df[x_col]!=maxValue].copy()\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "    df.sort_values(by=[x_col],ascending=False,inplace=True)\n",
    "    f, ax = plt.subplots(figsize = (30,15))\n",
    "    p = sns.barplot(data=df, y = y_col,x=x_col).set_title(title)\n",
    "    \n",
    "    \n",
    "    \n",
    "    #looping through every bar\n",
    "    for p in ax.patches:\n",
    "        _x = p.get_x() + p.get_width() + 0.4\n",
    "        _y = p.get_y() + p.get_height()\n",
    "        # Storing the text to be output as data label in 'value', p.get_width() retrieves the value\n",
    "        value = ' '+ str(int(p.get_width()))+' / '+str(round((int(p.get_width())/maxValue)*100,2))+'%'\n",
    "        ax.text(_x, _y, value, ha=\"left\")\n",
    "                    \n",
    "    p.figure.savefig(path)\n",
    "    \n",
    "# meta = createMetaTable(df)\n",
    "# createHorizontalBar(meta,'Fill Amount','Column Name','PRO Data Gaps','exports/191002/PRO Data Gaps.png',False)\n",
    "# createHorizontalBar(meta,'Number Unique','Column Name','PRO Unique Count','exports/191002/PRO Unique Count.png',False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert date to datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master['Award Date']=pd.to_datetime(master['Award Date'],infer_datetime_format=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pandas Profile Profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas_profiling as pp\n",
    "# df = pd.read_excel('DESKTOP/EIFNAISDAF.XLSX')\n",
    "profile = pp.ProfileReport(df)\n",
    "profile.to_file(outputfile=\"../references/profiles/Profile_.html\")\n",
    "\n",
    "# JSON Output\n",
    "# pp.describe_df(DF_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Count number of occurances of a character"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "df_new['Email Count'] = df_new['Email Address'].str.count('@')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Mean Median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "construction.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter Null amounts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def filter_null_shipment(x):\n",
    "    if(x==0):\n",
    "        return True\n",
    "    elif(x!=x):\n",
    "        return False\n",
    "    else: \n",
    "        reurn False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "po['Null Shipment Amount'] = po['Actual Shipment Amount'].apply(filter_null_shipment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataframe Value Count for Excel Graphing Bar Info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in data.columns: \n",
    "    print(col) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in po.columns:\n",
    "    col_name = str(col)\n",
    "    temp = po.dropna(subset=[col_name])\n",
    "    print(len(temp[col_name]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make lower case upper case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "master_2['Email'] = master_2['Email'].str.lower"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zip Code Merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zipcodes = pd.read_excel('../zip_code_database-171005.xls')\n",
    "\n",
    "zipcodes.rename(columns={'zip':'Zip','county':'County','state':'State Merged'},inplace=True)\n",
    "zipcodes['Zip'] = zipcodes['Zip'].astype(str)\n",
    "zipcodes=zipcodes[['Zip','County','State Merged']].copy()\n",
    "\n",
    "def clean_zip(string):\n",
    "    string = str(string)[:5]\n",
    "    \n",
    "    if(len(string)==2):\n",
    "        return '000'+string\n",
    "    elif(len(string)==3):\n",
    "        return '00'+string\n",
    "    elif(len(string)==4):\n",
    "        return '0'+string\n",
    "    else:\n",
    "        return string\n",
    "    \n",
    "df['Zip'] = df['Zip'].apply(clean_zip)\n",
    "zipcodes['Zip'] = zipcodes['Zip'].apply(clean_zip)\n",
    "df = df.merge(zipcodes,on='Zip',how='left')\n",
    "df['State'] = df['State Merged']\n",
    "df[df['County'].isnull()]['Zip'].value_counts()\n",
    "#df.loc[df['County'].isnull(),'MFD'] = 'Invalid Zip'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "zipcodes = pd.read_excel('../zip_code_database-171005.xls')\n",
    "zipcodes.rename(columns={'zip':'Zip','county':'County'},inplace=True)\n",
    "zipcodes['Zip'] = zipcodes['Zip'].astype(str)\n",
    "zipcodes=zipcodes[['Zip','County']].copy()\n",
    "def clean_zip(string):\n",
    "    string = str(string)[:5]\n",
    "    \n",
    "    if(len(string)<5):\n",
    "        return np.nan\n",
    "    else:\n",
    "        return string\n",
    "\n",
    "vendors['Zip'] = vendors['Zip'].apply(clean_zip)\n",
    "vendors = vendors.merge(zipcodes,on='Zip',how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assigning values to a column based on values of another column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "po.loc[po['Null Shipment Amount']==True,'MFD'] = 'Null or Zero Shipment Amount'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merging on shared column updating old values with new values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "esther_work = esther_work[['Business Name','New Ethnicity']]\n",
    "master = master.merge(esther_work, on='Business Name',how='left')\n",
    "#This assigns any new values that were brought in to overwrite the old values\n",
    "master.loc[master['New Ethnicity'].notnull(),'Ethnicity'] = master['New Ethnicity']\n",
    "master = master.drop(labels=['New Ethnicity'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Changing default number of rows columns displayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows',2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns',None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert pdf to dataframe excel csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tabula\n",
    "\n",
    "df = read_pdf('data/asdfsadf.pdf')\n",
    "tabula.convert_into('inout path',output_format=\"csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Format Phone Numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def phone_format(n):                                                                                                                                  \n",
    "    if(n[0].isdigit()==False):\n",
    "        return n\n",
    "    else:\n",
    "        return format(int(n[:-1]), \",\").replace(\",\", \"-\") + n[-1] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Drop Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "po.drop_duplicates(subset='Po Supplier Name',inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Drop Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "df.drop(['B', 'C'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pivot Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "po_pivot = pd.pivot_table(po,index=['Po Supplier Name'], values='ID', aggfunc=pd.Series.nunique)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flattening the pivot table so it is usable as a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "construction_pivot = pd.DataFrame(construction_pivot.to_records())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "po_pivot.sort_values(by=['ID'],ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create MFD Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def mfd_zip(df):\n",
    "    for index, row in df.iterrows():\n",
    "        if(df.iloc[index]['Zip'][0].isalpha()):\n",
    "            df.at[index,'MFD'] = 'Out of Country Zip'\n",
    "        elif(df.iloc[index]['Zip'] == '00000'):\n",
    "            df.at[index,'MFD'] = 'Invalid Zip Code'\n",
    "        elif(df.iloc[index]['Zip'] == '00nan'):\n",
    "            df.at[index,'MFD'] = 'Invalid Zip Code'\n",
    "        elif(df.iloc[index]['Zip'] == '00820'):\n",
    "            df.at[index,'County'] = 'Saint Croix'\n",
    "        elif(df.iloc[index]['Zip'] == '07623'):\n",
    "            df.at[index,'Zip'] = '07632'\n",
    "            df.at[index,'City'] = 'Englewood Cliffs'\n",
    "            df.at[index,'County'] = 'Bergen County'\n",
    "            df.at[index,'State'] = 'NJ'\n",
    "        elif(df.iloc[index]['Zip'] == '20802'):\n",
    "            df.at[index,'Zip'] = '80202'\n",
    "            df.at[index,'City'] = 'Denver'\n",
    "            df.at[index,'County'] = 'Denver County'\n",
    "            df.at[index,'State'] = 'CO'\n",
    "        elif(df.iloc[index]['Zip'] == '30057'):\n",
    "            df.at[index,'MFD'] = 'Invalid Zip Code'\n",
    "        elif(df.iloc[index]['Zip'] == '37418'):\n",
    "            df.at[index,'Zip'] = '37408'\n",
    "            df.at[index,'City'] = 'Chattanooga'\n",
    "            df.at[index,'County'] = 'Hamilton County'\n",
    "            df.at[index,'State'] = 'TN'\n",
    "        elif(df.iloc[index]['Zip'] == '37440'):\n",
    "            df.at[index,'Zip'] = '37410'\n",
    "            df.at[index,'City'] = 'Chattanooga'\n",
    "            df.at[index,'County'] = 'Hamilton County'\n",
    "            df.at[index,'State'] = 'TN'\n",
    "        elif(df.iloc[index]['Zip'] == '37446'):\n",
    "            df.at[index,'Zip'] = '37416'\n",
    "            df.at[index,'City'] = 'Chattanooga'\n",
    "            df.at[index,'County'] = 'Hamilton County'\n",
    "            df.at[index,'State'] = 'TN'\n",
    "        elif(df.iloc[index]['Zip'] == '37463'):\n",
    "            df.at[index,'Zip'] = '37403'\n",
    "            df.at[index,'City'] = 'Chattanooga'\n",
    "            df.at[index,'County'] = 'Hamilton County'\n",
    "            df.at[index,'State'] = 'TN'\n",
    "        elif(df.iloc[index]['Zip'] == '37471'):\n",
    "            df.at[index,'Zip'] = '37421'\n",
    "            df.at[index,'City'] = 'Chattanooga'\n",
    "            df.at[index,'County'] = 'Hamilton County'\n",
    "            df.at[index,'State'] = 'TN'\n",
    "        elif(df.iloc[index]['Zip'] == '38309'):\n",
    "            df.at[index,'Zip'] = '37309'\n",
    "            df.at[index,'City'] = 'Calhoun'\n",
    "            df.at[index,'County'] = 'McMinn County'\n",
    "            df.at[index,'State'] = 'TN'\n",
    "        elif(df.iloc[index]['Zip'] == '41247'):\n",
    "            df.at[index,'MFD'] = 'Invalid Zip Code'\n",
    "        elif(df.iloc[index]['Zip'] == '49536'):\n",
    "            df.at[index,'MFD'] = 'Out of Country Zip'\n",
    "        elif(df.iloc[index]['Zip'] == '60904'):\n",
    "            df.at[index,'Zip'] = '30904'\n",
    "            df.at[index,'City'] = 'Augusta'\n",
    "            df.at[index,'County'] = 'Richmond County'\n",
    "            df.at[index,'State'] = 'GA'\n",
    "        elif(df.iloc[index]['Zip'] == '70100'):\n",
    "            df.at[index,'MFD'] = 'Out of Country Zip'   \n",
    "        #else:\n",
    "            #df.at[index,'MFD'] = np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Excel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Delete Blank Rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "https://www.excel-easy.com/examples/delete-blank-rows.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
